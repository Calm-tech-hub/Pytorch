{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1577x790 at 0x28D3689F670>\n",
      "(1577, 790)\n"
     ]
    }
   ],
   "source": [
    "#关注输入，输出，作用\n",
    "'''\n",
    "PIL -> Image.open()\n",
    "tensor -> ToTensor()\n",
    "narrays -> cv.imread() \n",
    "'''\n",
    "img = Image.open(r\"img\\壁纸.jpg\")\n",
    "print(img)\n",
    "print(img.size)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Totensor的使用\n",
    "trans_totensor = transforms.ToTensor()#这是一个类\n",
    "img_tensor = trans_totensor(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1577x790 at 0x28D3692B2E0>\n",
      "tensor([[[0.5373, 0.5490, 0.5569,  ..., 0.4314, 0.6431, 0.8392],\n",
      "         [0.4431, 0.4431, 0.4471,  ..., 0.2745, 0.3961, 0.5686],\n",
      "         [0.4784, 0.4863, 0.4902,  ..., 0.2941, 0.3176, 0.2980],\n",
      "         ...,\n",
      "         [0.7412, 0.7412, 0.7412,  ..., 0.3686, 0.3686, 0.3686],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.9333, 0.9333, 0.9333],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.5333, 0.5451, 0.5529,  ..., 0.4235, 0.6353, 0.8314],\n",
      "         [0.4275, 0.4392, 0.4431,  ..., 0.2667, 0.3882, 0.5608],\n",
      "         [0.4627, 0.4706, 0.4745,  ..., 0.2863, 0.3098, 0.2902],\n",
      "         ...,\n",
      "         [0.6745, 0.6745, 0.6745,  ..., 0.3137, 0.3137, 0.3137],\n",
      "         [0.9569, 0.9569, 0.9569,  ..., 0.8863, 0.8863, 0.8863],\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.9569, 0.9569, 0.9569]],\n",
      "\n",
      "        [[0.5255, 0.5373, 0.5451,  ..., 0.4275, 0.6392, 0.8353],\n",
      "         [0.4235, 0.4314, 0.4353,  ..., 0.2706, 0.3922, 0.5647],\n",
      "         [0.4588, 0.4667, 0.4706,  ..., 0.2902, 0.3137, 0.2941],\n",
      "         ...,\n",
      "         [0.4627, 0.4627, 0.4627,  ..., 0.2118, 0.2118, 0.2118],\n",
      "         [0.7843, 0.7843, 0.7843,  ..., 0.8314, 0.8314, 0.8314],\n",
      "         [0.8275, 0.8275, 0.8275,  ..., 0.9176, 0.9176, 0.9176]]])\n",
      "torch.Size([3, 790, 1577])\n",
      "torch.Size([3, 790, 1577])\n"
     ]
    }
   ],
   "source": [
    "#transform工具箱 totensor resize等等tools，  图片-----(transforms)---->结果\n",
    "\n",
    "#通过 transform.totensor 去解决两个问题\n",
    "#1、transform如何使用\n",
    "#2、为什么需要tensor数据类型\n",
    "\n",
    "image_path = r\"img\\壁纸.jpg\"\n",
    "img = Image.open(image_path)\n",
    "print(img)\n",
    "\n",
    "tensor_trans = transforms.ToTensor()#这是实例化一个类,创建一个具体的工具tool result = tool(input)\n",
    "\n",
    "tensor_img = tensor_trans(img)\n",
    "print(tensor_img)\n",
    "print(tensor_img.shape)\n",
    "print(tensor_img.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "cv_img = cv2.imread(image_path)\n",
    "print(type(cv_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "writer.add_image(\"Tensor_img\",tensor_img)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__Helloxkm\n",
      "helloxkm\n"
     ]
    }
   ],
   "source": [
    "#常用transforms\n",
    "#__call__的用法\n",
    "class Person:\n",
    "    def __call__(self,name):\n",
    "        print(\"__call__\" + \"Hello\"+name)\n",
    "\n",
    "    def hello(self,name):\n",
    "        print(\"hello\" + name)\n",
    "\n",
    "\n",
    "person = Person()\n",
    "person(\"xkm\")#__xxx__这种内置函数可以直接传入参数调用，可以不用'.'这个符号来进行调用\n",
    "person.hello(\"xkm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToTensor\n",
    "writer = SummaryWriter(\"logs\")\n",
    "img = Image.open(r\"img\\壁纸.jpg\")\n",
    "img_ToTensor = transforms.ToTensor()\n",
    "tensor_img = img_ToTensor(img)#输入为PIL或者是np.arrsy\n",
    "\"\"\"Convert a PIL Image or ndarray to tensor and scale the values accordingly.\n",
    "\n",
    "    This transform does not support torchscript.\n",
    "\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
    "    or if the numpy.ndarray has dtype = np.uint8\n",
    "\n",
    "    In the other cases, tensors are returned without scaling.\n",
    "\n",
    "    .. note::\n",
    "        Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n",
    "        transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n",
    "\n",
    "    .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation\n",
    "\"\"\"\n",
    "writer.add_image(\"seven\",tensor_img,1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0745)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n可以通过这种方法来改变tensor的值的范围,比如一开始[0,1]----(2*x-1)----->[-1,1]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize\n",
    "trans_norm = transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "Normalized_tensor = trans_norm(tensor_img)#输入torch.tensor\n",
    "\"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    This transform does not support PIL Image.\n",
    "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
    "    channels, this transform will normalize each channel of the input\n",
    "    ``torch.*Tensor`` i.e.,\n",
    "    计算公式：``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "\n",
    "    .. note::\n",
    "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "\n",
    "\"\"\"\n",
    "print(Normalized_tensor[0][0][0])\n",
    "'''\n",
    "可以通过这种方法来改变tensor的值的范围,比如一开始[0,1]----(2*x-1)----->[-1,1]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_image(\"Normalized_pic\",Normalized_tensor)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1577, 790)\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x28D794356A0>\n"
     ]
    }
   ],
   "source": [
    "#Resize\n",
    "print(img.size)\n",
    "trans_resize = transforms.Resize((512,512))\n",
    "\"\"\"Resize the input image to the given size.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
    "\n",
    "    .. warning::\n",
    "        The output image might be different depending on its type: when downsampling, the interpolation of PIL images\n",
    "        and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences\n",
    "        in the performance of a network. Therefore, it is preferable to train and serve a model with the same input\n",
    "        types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors\n",
    "        closer.\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (h, w), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size).\n",
    "\n",
    "            .. note::\n",
    "                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n",
    "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
    "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
    "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``,\n",
    "            ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported.\n",
    "            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.\n",
    "        max_size (int, optional): The maximum allowed for the longer edge of\n",
    "            the resized image: if the longer edge of the image is greater\n",
    "            than ``max_size`` after being resized according to ``size``, then\n",
    "            the image is resized again so that the longer edge is equal to\n",
    "            ``max_size``. As a result, ``size`` might be overruled, i.e. the\n",
    "            smaller edge may be shorter than ``size``. This is only supported\n",
    "            if ``size`` is an int (or a sequence of length 1 in torchscript\n",
    "            mode).\n",
    "        antialias (bool, optional): Whether to apply antialiasing.\n",
    "            It only affects **tensors** with bilinear or bicubic modes and it is\n",
    "            ignored otherwise: on PIL images, antialiasing is always applied on\n",
    "            bilinear or bicubic modes; on other modes (for PIL images and\n",
    "            tensors), antialiasing makes no sense and this parameter is ignored.\n",
    "            Possible values are:\n",
    "\n",
    "            - ``True``: will apply antialiasing for bilinear or bicubic modes.\n",
    "              Other mode aren't affected. This is probably what you want to use.\n",
    "            - ``False``: will not apply antialiasing for tensors on any mode. PIL\n",
    "              images are still antialiased on bilinear or bicubic modes, because\n",
    "              PIL doesn't support no antialias.\n",
    "            - ``None``: equivalent to ``False`` for tensors and ``True`` for\n",
    "              PIL images. This value exists for legacy reasons and you probably\n",
    "              don't want to use it unless you really know what you are doing.\n",
    "\n",
    "            The current default is ``None`` **but will change to** ``True`` **in\n",
    "            v0.17** for the PIL and Tensor backends to be consistent.\n",
    "    \"\"\"\n",
    "\n",
    "img_resize = trans_resize(img)#输入可以是PIL\n",
    "print(img_resize)\n",
    "\n",
    "img_resize = trans_totensor(img_resize)\n",
    "\n",
    "writer.add_image(\"Resized_pic\",img_resize)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compose\n",
    "trans_resize_2 = transforms.Resize(512)\n",
    "trans_compose = transforms.Compose([trans_resize_2,trans_totensor])#transforms的compose的参数是一个列表，[1,2,3,4,....],数据类型是transforms类型\n",
    "img_resize_2 = trans_compose(img)\n",
    "writer.add_image(\"Composed_pic\",img_resize_2)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random_crop\n",
    "trans_random = transforms.RandomCrop(512)\n",
    "\"\"\"Crop the given image at a random location.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions,\n",
    "    but if non-constant padding is used, the input is expected to have at most 2 leading dimensions\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is None. If a single int is provided this\n",
    "            is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
    "            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
    "            this is the padding for the left, top, right and bottom borders respectively.\n",
    "\n",
    "            .. note::\n",
    "                In torchscript mode padding as single int is not supported, use a sequence of\n",
    "                length 1: ``[padding, ]``.\n",
    "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "            desired size to avoid raising an exception. Since cropping is done\n",
    "            after padding, the padding seems to be done at a random offset.\n",
    "        fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
    "            length 3, it is used to fill R, G, B channels respectively.\n",
    "            This value is only used when the padding_mode is constant.\n",
    "            Only number is supported for torch Tensor.\n",
    "            Only int or tuple value is supported for PIL Image.\n",
    "        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n",
    "            Default is constant.\n",
    "\n",
    "            - constant: pads with a constant value, this value is specified with fill\n",
    "\n",
    "            - edge: pads with the last value at the edge of the image.\n",
    "              If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n",
    "\n",
    "            - reflect: pads with reflection of image without repeating the last value on the edge.\n",
    "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
    "              will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
    "\n",
    "            - symmetric: pads with reflection of image repeating the last value on the edge.\n",
    "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
    "              will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
    "    \"\"\"\n",
    "trans_compose_2 = transforms.Compose([trans_random,trans_totensor])\n",
    "for i in range(10):\n",
    "    writer.add_image(\"RandomCroped pic\",trans_compose_2(img),i)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "查看源文档的类\n",
    "1、关注输入和输出\n",
    "2、多看官方文档\n",
    "3、通过print(type())来查看输出的数据类型\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
